<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Softwares on Paul C. Lambert</title>
    <link>/software/</link>
    <description>Recent content in Softwares on Paul C. Lambert</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Paul C Lambert</copyright>
    <lastBuildDate>Sun, 27 Aug 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/software/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>stcrprep - computational benefits</title>
      <link>/software/stcrprep/computational_benefits/</link>
      <pubDate>Sun, 27 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/software/stcrprep/computational_benefits/</guid>
      <description>&lt;p&gt;When using &lt;code&gt;stcrprep&lt;/code&gt; there are some computational benefits when compared to using Stata&amp;rsquo;s inbuilt &lt;code&gt;stcrreg&lt;/code&gt;. One reason for this is that everytime you fit a model using &lt;code&gt;stcrreg&lt;/code&gt; you the probability of censoring weights are calculated and the data must be expanded (in the background) when maximising the likelihood. When using &lt;code&gt;stcrprep&lt;/code&gt; you only need to do this once.&lt;/p&gt;

&lt;p&gt;I have run some timings. If I fit a simple model to the embt1 data with risk score as the only covariate (2 dummy variables) then these are the timings no my current work laptop (Intel i5 - running Stata 15 MP2).&lt;/p&gt;

&lt;p&gt;First I load and &lt;code&gt;stset&lt;/code&gt; the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. use https://www.pclambert.net/data/ebmt1_stata.dta, clear

. stset time, failure(status==1) scale(365.25) id(patid) noshow

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, &lt;code&gt;stcrreg&lt;/code&gt; can be used&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. timer clear

. timer on 1

. stcrreg i.score, compete(status==2) nolog noshow

Competing-risks regression                       No. of obs       =      1,977
                                                 No. of subjects  =      1,977
Failure event  : status == 1                     No. failed       =        456
Competing event: status == 2                     No. competing    =        685
                                                 No. censored     =        836

                                                 Wald chi2(2)     =       9.87
Log pseudolikelihood = -3333.3217                Prob &amp;gt; chi2      =     0.0072

                              (Std. Err. adjusted for 1,977 clusters in patid)
------------------------------------------------------------------------------
             |               Robust
          _t |        SHR   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
       score |
Medium risk  |   1.271221   .1554323     1.96   0.050     1.000333    1.615465
  High risk  |   1.769853   .3238535     3.12   0.002     1.236465    2.533337
------------------------------------------------------------------------------

. timer off 1

. timer list
   1:     16.21 /        1 =      16.2060

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This takes 16.2 seconds to fit.&lt;/p&gt;

&lt;p&gt;I now reload and &lt;code&gt;stset&lt;/code&gt; the data, but this time declaring both &lt;code&gt;status=1&lt;/code&gt; and &lt;code&gt;status=2&lt;/code&gt; as events.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. use https://www.pclambert.net/data/ebmt1_stata.dta, clear

. stset time, failure(status==1,2) scale(365.25) id(patid)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now run &lt;code&gt;stcrprep&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. timer on 2

. stcrprep, events(status) keep(score) trans(1)   

. timer off 2

. timer list 2
   2:      4.48 /        1 =       4.4820

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This takes  4.5 seconds to run. However, this only restructures the data and calculates the weights. To fit the model, we first generate the event indicator and  use &lt;code&gt;stset&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. gen      event = status == failcode

. stset tstop [iw=weight_c], failure(event) enter(tstart) 

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We use &lt;code&gt;stcox&lt;/code&gt; to fit the model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. timer on 3

. stcox i.score

         failure _d:  event
   analysis time _t:  tstop
  enter on or after:  time tstart
             weight:  [iweight=weight_c]

Iteration 0:   log likelihood = -3338.1244
Iteration 1:   log likelihood = -3333.4173
Iteration 2:   log likelihood = -3333.3113
Iteration 3:   log likelihood = -3333.3112
Refining estimates:
Iteration 0:   log likelihood = -3333.3112

Cox regression -- Breslow method for ties

No. of subjects =       72,880                  Number of obs    =      72,880
No. of failures =          456
Time at risk    =   6026.27434
                                                LR chi2(2)       =        9.63
Log likelihood  =   -3333.3112                  Prob &amp;gt; chi2      =      0.0081

------------------------------------------------------------------------------
          _t | Haz. Ratio   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
       score |
Medium risk  |   1.271235   .1593392     1.91   0.056     .9943389    1.625238
  High risk  |   1.769899   .3219273     3.14   0.002     1.239148     2.52798
------------------------------------------------------------------------------

. timer off 3

. timer list
   1:     16.21 /        1 =      16.2060
   2:      4.48 /        1 =       4.4820
   3:      1.06 /        1 =       1.0630

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This takes  1.1 seconds to run giving a combined total of  5.5 seconds. What is important is that if we want to fit other models (including other covariates etc), then we do not need to run &lt;code&gt;stset&lt;/code&gt; again.&lt;/p&gt;

&lt;p&gt;To assess the time on larger data I have expanded the data by 20 times and added a small random number to each time, so that there are no ties. I used the following code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;expand 20
replace time = time + runiform()*0.0001
replace patid = _n
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This leads to 19,770 indviduals in the analysis. The fact that there are no ties is perhaps a little unrealistic in a dataset this size, but this is still a usefull assessment of computational speed. The same analysis as above on this larger dataset gave the following times.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;command&lt;/th&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;stcrreg&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;2066.3 seconds&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;stcrprep&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;890.2 seconds&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;stcox&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;46.1 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I think this really highlights the benfits of restructuring the data and using &lt;code&gt;stcox&lt;/code&gt; in terms of computational time. Unless there is need to recalculate the probability of censoring weights, there is no need to do this every time you fit a model. Thus, in this case an &lt;code&gt;stcrreg&lt;/code&gt; model takes almost 35 minutes, whilst the same model using &lt;code&gt;stcox&lt;/code&gt; after using &lt;code&gt;stcrprep&lt;/code&gt;takes only 46 seconds.&lt;/p&gt;

&lt;p&gt;It is worthwhile noting that Stata&amp;rsquo;s implementation of Fine and Grays proportional subhazards model using &lt;code&gt;stcrreg&lt;/code&gt; seems particularly slow. If I fit the model in R using &lt;code&gt;crr&lt;/code&gt; the model fitted to the expanded data it only takes 370 seconds compared to 2066 in Stata.&lt;/p&gt;

&lt;p&gt;There are other benefits with using &lt;code&gt;stcox&lt;/code&gt; to fit the subhazards model, mainly because we can now use many of the other commands and extensions associated with &lt;code&gt;stcox&lt;/code&gt;. I will discuss these in other tutorials.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>stcrprep</title>
      <link>/software/stcrprep/</link>
      <pubDate>Mon, 21 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/software/stcrprep/</guid>
      <description>

&lt;p&gt;stcrprep prepares data for estimating and modelling cause-specific cumulative incidence functions using time-dependent weights. Once the data has been prepared and the weights incorporated using &lt;code&gt;stset&lt;/code&gt; it is possible to obtain a graph of the non-parametric estimates of the cause-specific cumulative incidence function using &lt;code&gt;sts graph&lt;/code&gt;.  In addition a model that estimates subhazard ratios (equivalent to the Fine and Gray model) can be fitted using &lt;code&gt;stcox&lt;/code&gt;. It is also possible to fit parametric models to directly estimate the cause-specific CIF (my main reason for developing the command).&lt;/p&gt;

&lt;p&gt;Below are some simple examples of using &lt;code&gt;stcrprep&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;

&lt;h3 id=&#34;non-and-semi-parametric-methods&#34;&gt;Non and semi parametric methods&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/software/stcrprep/nonparametriccif/&#34;&gt;Using sts graph for cause-specific CIFs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Using &lt;code&gt;stcox&lt;/code&gt; instead of &lt;code&gt;stcrreg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/software/stcrprep/computational_benefits/&#34;&gt;Computational benefits of using &lt;code&gt;stcrprep&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Schoenfeld residuals&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;parametric-models&#34;&gt;Parametric models&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Using &lt;code&gt;stpm2&lt;/code&gt; to model the cause-specific CIF&lt;/li&gt;
&lt;li&gt;Alternative link functions.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>stpm2</title>
      <link>/software/stpm2/</link>
      <pubDate>Mon, 21 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/software/stpm2/</guid>
      <description>

&lt;p&gt;&lt;code&gt;stpm2&lt;/code&gt; fits flexible parametric survival models. These models use splines to model some transformation of the survial function. The most common is the  $\log[-\log[S(t)]]$ link function, which fits proportional hazards models.&lt;/p&gt;

&lt;p&gt;I have added some examples and aim to add to these.&lt;/p&gt;

&lt;h2 id=&#34;proportional-hazards-models&#34;&gt;Proportional hazards models&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/software/stpm2/comparewithcox/&#34;&gt;Comparison with a Cox model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;simple simulation study to show agreement with Cox model.&lt;/li&gt;
&lt;li&gt;predicting hazard and survival functions (use of the &lt;code&gt;timevar()&lt;/code&gt; option)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/software/stpm2/sensitivity_analysis/&#34;&gt;Sensitivity analysis for the number of knots&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;time-dependent-effects-non-proportional-hazards&#34;&gt;Time-dependent effects (non proportional hazards)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;non-proportional hazards&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;relative-survival&#34;&gt;Relative survival&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;a simple relative survival model&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;alternative-link-functions&#34;&gt;Alternative link functions&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;the logistic, probit and Aranda-Ordaz link functions.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Proportional hazards models in stpm2</title>
      <link>/software/stpm2/comparewithcox/</link>
      <pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/software/stpm2/comparewithcox/</guid>
      <description>

&lt;h1 id=&#34;proportional-hazards-model&#34;&gt;Proportional hazards model&lt;/h1&gt;

&lt;p&gt;We first load the example breast cancer data data using &lt;code&gt;webuse&lt;/code&gt; and then use &lt;code&gt;stset&lt;/code&gt; to declare the survival time and event indicator.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. webuse brcancer, clear
(German breast cancer data)

. stset rectime, f(censrec==1) scale(365.24)

     failure event:  censrec == 1
obs. time interval:  (0, rectime]
 exit on or before:  failure
    t for analysis:  time/365.24

------------------------------------------------------------------------------
        686  total observations
          0  exclusions
------------------------------------------------------------------------------
        686  observations remaining, representing
        299  failures in single-record/single-failure data
  2,112.036  total analysis time at risk and under observation
                                                at risk from t =         0
                                     earliest observed entry t =         0
                                          last observed exit t =  7.280145

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;scale(365.25)&lt;/code&gt; option converts the times recorded in days to years.&lt;/p&gt;

&lt;p&gt;A standard Cox proportional hazards model can be defined as follows,&lt;/p&gt;

&lt;p&gt;$$
h_i(t|\mathbf{x}_i)=h_0(t)\exp\left(\mathbf{x}_i\boldsymbol{\beta}\right)
$$&lt;/p&gt;

&lt;p&gt;A key point about the Cox model is that we do not estimate the baseline hazard, $h_0(t)$, as this cancels out in the partial likelihood, so we only estimate the relative effects, i.e. hazard ratios.&lt;/p&gt;

&lt;p&gt;We can now fit a Cox model in Stata with &lt;code&gt;hormon&lt;/code&gt; as the only covariate.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. stcox hormon, 

         failure _d:  censrec == 1
   analysis time _t:  rectime/365.24

Iteration 0:   log likelihood = -1788.1731
Iteration 1:   log likelihood =  -1783.774
Iteration 2:   log likelihood =  -1783.765
Iteration 3:   log likelihood =  -1783.765
Refining estimates:
Iteration 0:   log likelihood =  -1783.765

Cox regression -- Breslow method for ties

No. of subjects =          686                  Number of obs    =         686
No. of failures =          299
Time at risk    =  2112.035922
                                                LR chi2(1)       =        8.82
Log likelihood  =    -1783.765                  Prob &amp;gt; chi2      =      0.0030

------------------------------------------------------------------------------
          _t | Haz. Ratio   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      hormon |   .6949616   .0869009    -2.91   0.004      .543905    .8879705
------------------------------------------------------------------------------

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will now fit a flexible parametric survival model. Here are model is on the log &lt;em&gt;cumulative&lt;/em&gt; hazard scale, so our model is defined using uppercase H rather than lowercase h.&lt;/p&gt;

&lt;p&gt;$$
H_i(t|\mathbf{x}_i)=H_0(t)\exp\left(\mathbf{x}_i\boldsymbol{\beta}\right)
$$&lt;/p&gt;

&lt;p&gt;Do we have to worry about the switch from hazard function to cumulative hazard function? Well, the answer is &amp;ldquo;No&amp;rdquo; as if we have proportional hazards we also have proportional cumulative hazards.&lt;/p&gt;

&lt;p&gt;In the flexible parametric survival model we estimate the baseline using restriced cubic splines. So we need additional parameters to estimate the baseline (the log cumulative hazard in this case).  The linear predictor is,&lt;/p&gt;

&lt;p&gt;$$
\ln[H(t|\mathbf{x}_i)] = \eta_i(t) = s\left(\ln(t)|\boldsymbol{\gamma}, \mathbf{k}_{0}\right) + \mathbf{x}_i \boldsymbol{\beta}
$$&lt;/p&gt;

&lt;p&gt;where $s\left(\ln(t)|\boldsymbol{\gamma}, \mathbf{k}_{0}\right)$ is a restriced cubic spline function of log(time).&lt;/p&gt;

&lt;p&gt;We now fit this model in Stata using &lt;code&gt;stpm2&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. stpm2 hormon, df(3) scale(hazard) eform

Iteration 0:   log likelihood = -671.75275  
Iteration 1:   log likelihood = -670.39949  
Iteration 2:   log likelihood = -670.39239  
Iteration 3:   log likelihood = -670.39239  

Log likelihood = -670.39239                     Number of obs     =        686

------------------------------------------------------------------------------
             |     exp(b)   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
xb           |
      hormon |   .6966754   .0870015    -2.89   0.004     .5454206    .8898757
       _rcs1 |   4.931928   .6329089    12.43   0.000     3.835156    6.342354
       _rcs2 |   1.786812   .2092654     4.96   0.000     1.420329    2.247857
       _rcs3 |   .9519031     .03275    -1.43   0.152     .8898306    1.018306
       _cons |   .3032836   .0247012   -14.65   0.000     .2585366    .3557753
------------------------------------------------------------------------------
Note: Estimates are transformed only in the first equation.

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have used three options. The &lt;code&gt;df(3)&lt;/code&gt; option requests there to be 3 restricted cubic spline parameters (4 knots). These are at the default knot locations, which are at evenly spaced centiles of the uncensored event times. The &lt;code&gt;scale()&lt;/code&gt; option defines the link function and &lt;code&gt;scale(hazard)&lt;/code&gt; asks for a log(-log) link function, i.e. our linear predictor is on the log cumulative hazard scale. The &lt;code&gt;eform&lt;/code&gt; option means that the coefficients will be exponentiated.&lt;/p&gt;

&lt;p&gt;The key point here is the similarity between the hazard ratios form the two models. This is nearly always the case. I will try to convince anyone who does not believe this in future posts.&lt;/p&gt;

&lt;p&gt;A sensible question is, &lt;em&gt;if we get the same anwers, why not just fit a Cox model&lt;/em&gt;?  Well, if all you want is a single hazard ratio and proportional hazards is a reasonable assumption then I agree with you. However, as I will show in other examples, there are many advantages of the parametric approach.&lt;/p&gt;

&lt;p&gt;There are a number of issues that people may raise. This include how many knots to use and where to put the knots. I will cover these in future tutorials.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sensitivity analysis to number of knots (proportional hazards)</title>
      <link>/software/stpm2/sensitivity_analysis/</link>
      <pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/software/stpm2/sensitivity_analysis/</guid>
      <description>

&lt;h1 id=&#34;sensitivity-analysis&#34;&gt;Sensitivity Analysis&lt;/h1&gt;

&lt;p&gt;We first load the example Rotterdam breast cancer data (rott2b.dta)  and then use &lt;code&gt;stset&lt;/code&gt; to declare the survival time (relapse free survival) and event indicator. Follow-up is restricted to 5 years using the &lt;code&gt;exit()&lt;/code&gt; option.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. use https://www.pclambert.net/data/rott2b, clear
(Rotterdam breast cancer data (augmented with cause of death))

. stset rf, f(rfi==1) scale(12) exit(time 60)

     failure event:  rfi == 1
obs. time interval:  (0, rf]
 exit on or before:  time 60
    t for analysis:  time/12

------------------------------------------------------------------------------
      2,982  total observations
          0  exclusions
------------------------------------------------------------------------------
      2,982  observations remaining, representing
      1,181  failures in single-record/single-failure data
 11,130.825  total analysis time at risk and under observation
                                                at risk from t =         0
                                     earliest observed entry t =         0
                                          last observed exit t =         5

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I will use different degrees of freedom for the baseline. The easiest way to do this is in a loop. The following code fits between 1 and 6 df, predicts the baseline hazard and survival functions and stores each model (using &lt;code&gt;estimates store&lt;/code&gt;).  I use &lt;code&gt;quietly&lt;/code&gt; to suppress the output. I also generate a new time variable (&lt;code&gt;temptime&lt;/code&gt;) for the predictions, rather than use the default of &lt;code&gt;_t&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. range temptime 0 5 200
(2,782 missing values generated)

. forvalues i = 1/6 {
  2.         quietly stpm2 hormon, df(`i&#39;) scale(hazard) 
  3.         predict h0_df`i&#39;, hazard timevar(temptime) per(1000) zeros
  4.         predict s0_df`i&#39;, survival timevar(temptime) zeros
  5.         estimates store df`i&#39;
  6. }

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now compare the results from fitting the 6 different models.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. estimates table df*, keep(hormon) b(%6.5f) se(%6.5f) stats(AIC BIC) stfmt(%6.2f)

--------------------------------------------------------------------------
    Variable |   df1       df2       df3       df4       df5       df6    
-------------+------------------------------------------------------------
      hormon | 0.23312   0.22044   0.22038   0.22023   0.22020   0.22007  
             | 0.08642   0.08643   0.08643   0.08643   0.08643   0.08643  
-------------+------------------------------------------------------------
         AIC | 6362.72   6210.98   6212.43   6213.72   6213.75   6215.98  
         BIC | 6377.94   6231.28   6237.80   6244.17   6249.27   6256.57  
--------------------------------------------------------------------------
                                                              legend: b/se

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The log hazard ratios are very similar between the different models, particularly from 2df and above (using 1 df is equivalent to fitting a Weibull model). The standard errors are also very similar.&lt;/p&gt;

&lt;p&gt;The AIC and BIC can be used as an &lt;em&gt;informal&lt;/em&gt; guide (certainly not definitive) to the choice of model.  Both the AIC and BIC are lowest for the model with 2df.&lt;/p&gt;

&lt;p&gt;One of the advantages of using parametric models is the simplicity in which we can predict hazard, survival and other useful functions. In the loop when the 6 different models were fitted the baseline hazard and survival functions were also obtained. We can now compare these by plotting them.&lt;/p&gt;

&lt;p&gt;First, the baseline hazard functions. Note I used the &lt;code&gt;per(1000)&lt;/code&gt; option when I predicted the hazard functions to give the rate per 1000 person years.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. twoway  (line h0_df* temptime), ///
&amp;gt;                 ytitle(&amp;quot;hazard rate (per 1000 py)&amp;quot;) ///
&amp;gt;                 xtitle(&amp;quot;Years from surgery&amp;quot;) ///
&amp;gt;                 ylabel(,format(%2.0f) angle(h)) ///
&amp;gt;                 legend(order(1 &amp;quot;1 df&amp;quot; 2 &amp;quot;2 df&amp;quot; 3 &amp;quot;3df&amp;quot; 4 &amp;quot;4df&amp;quot; 5 &amp;quot;5df&amp;quot; 6 &amp;quot;6df&amp;quot;) ///
&amp;gt;                         ring(0) cols(1) pos(5))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The graph is shown below. The model with 1 df (equivalent to a Weibull model) stands out. The hazard function for the Weibull model is monotonic and so can&amp;rsquo;t pick up the turning point. There is fairly good agreement between the other models. Remember that the AIC and BIC indicate that models with 3df or more are over fitting.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/statasvg/ph_sensitivity_hazard.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now, using similar code we can plot the six baseline survival functions.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. twoway  (line s0_df* temptime), ///
&amp;gt;                 ytitle(&amp;quot;Survival function - S(t)&amp;quot;) ///
&amp;gt;                 xtitle(&amp;quot;Years from surgery&amp;quot;) ///
&amp;gt;                 ylabel(,format(%3.1f) angle(h)) ///
&amp;gt;                 legend(order(1 &amp;quot;1 df&amp;quot; 2 &amp;quot;2 df&amp;quot; 3 &amp;quot;3df&amp;quot; 4 &amp;quot;4df&amp;quot; 5 &amp;quot;5df&amp;quot; 6 &amp;quot;6df&amp;quot;) ///
&amp;gt;                         ring(0) cols(1) pos(1))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This graph is shown below and again the model with 1 df stands out, which is not surprising given the hazard function. However, there is excellent agreement between the remaining 5 models. In general, one sees better agreement when comparing survival functions as it is a cumulative measure the small differences seen the hazard functions cancel out.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/statasvg/ph_sensitivity_survival.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This has been a simple sensitivity analysis where I have assumed proportional hazards and only fitted a single covariate, but I hope that it shows how simple it is to try these things.&lt;/p&gt;

&lt;p&gt;What I see as the key point here is that even when selecting a too complex model (as indicated by the AIC and BIC) it makes little difference to the hazard ratio or the estimated hazard and survival functions. Of course one could argue that this is a single data set, but see Rutherord &lt;em&gt;et al.&lt;/em&gt; for a more detailed simulation study on the ability of these models to capture complex hazard functions.&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;Rutherford M.J., Crowther M.J., Lambert, P.C. The use of restricted cubic splines to approximate complex hazard functions in the analysis of time-to-event data: a simulation study. &lt;em&gt;Journal of Statistical Computation and Simulation&lt;/em&gt; 2015;&lt;strong&gt;4&lt;/strong&gt;:777–793&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>stcrprep - non parametric cause-specific CIFs</title>
      <link>/software/stcrprep/nonparametriccif/</link>
      <pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/software/stcrprep/nonparametriccif/</guid>
      <description>

&lt;p&gt;I will use the same data set I use in the &lt;em&gt;Stata Journal&lt;/em&gt; &lt;a href=&#34;http://www.stata-journal.com/article.html?article=st0471&#34;&gt;article&lt;/a&gt; on &lt;code&gt;stcrprep&lt;/code&gt;.
This comprises of 1977 patients from the European Blood and Marrow Transplantation (EBMT) registry who received an allogeneic bone
marrow transplantation. Time is measured in days from transplantation to either relapse
or death. There is only one covariate of interest, the EBMT risk score, which has been
categorized into 3 groups (low, medium and high risk). The data is available as part of
the mstate R package (de Wreede et al. 2011).&lt;/p&gt;

&lt;p&gt;First I load the data,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. use http://www.pclambert.net/data/ebmt1_stata.dta, clear
(Written by R.              )

. tab status

     status |      Freq.     Percent        Cum.
------------+-----------------------------------
   censored |        836       42.29       42.29
    relapse |        456       23.07       65.35
       died |        685       34.65      100.00
------------+-----------------------------------
      Total |      1,977      100.00

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The tabulation shows that of the 1,977 subjects, 836 were censored, 456 had a relapse and 686 had a death before relapse. Now we can &lt;code&gt;stset&lt;/code&gt; the data declaring both relapse and death as an event in the &lt;code&gt;failure()&lt;/code&gt; option.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. stset time, failure(status==1,2) scale(365.25) id(patid)

                id:  patid
     failure event:  status == 1 2
obs. time interval:  (time[_n-1], time]
 exit on or before:  failure
    t for analysis:  time/365.25

------------------------------------------------------------------------------
      1,977  total observations
          0  exclusions
------------------------------------------------------------------------------
      1,977  observations remaining, representing
      1,977  subjects
      1,141  failures in single-failure-per-subject data
  3,796.057  total analysis time at risk and under observation
                                                at risk from t =         0
                                     earliest observed entry t =         0
                                          last observed exit t =  8.454483

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to show how &lt;code&gt;stcrprep&lt;/code&gt; expands the data and calculates the probability of censoring weights for those with a competing event, I will list the data of a single individual before and after using &lt;code&gt;stcrprep&lt;/code&gt;. The listing is for subject 17 (&lt;code&gt;patid==17&lt;/code&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. list patid status _t0 _t _d if patid==17, noobs

  +---------------------------------------+
  | patid   status   _t0          _t   _d |
  |---------------------------------------|
  |    17     died     0   2.2888433    1 |
  +---------------------------------------+

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This subject died after 2.29 years and  before using &lt;code&gt;stcrprep&lt;/code&gt; has just has one row of
data.&lt;/p&gt;

&lt;p&gt;Next I use &lt;code&gt;stcrprep&lt;/code&gt; to restructure the data. The &lt;code&gt;events()&lt;/code&gt; option requires the variable defining all possible events and the censored value. The &lt;code&gt;trans()&lt;/code&gt; option gives the transitions of the events of interest; here we
are interested in the transitions to both relapse(&lt;code&gt;status=1&lt;/code&gt;) and death (&lt;code&gt;status=2&lt;/code&gt;); this is actually the default, but is shown here for clarity. The &lt;code&gt;keep()&lt;/code&gt; option is used to list variables to retain in the expanded data; usually any covariates that will be later analysed are included here. The &lt;code&gt;byg()&lt;/code&gt; option requests the censoring distribution to be estimated separately for the given groups. Since we are first going to obtain a separate
non-parametric estimate of the cause-specific CIF in each group, the byg() option will estimate the censoring distribution separately in each group.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. stcrprep, events(status) keep(score) trans(1 2) byg(score)

. di &amp;quot;There are &amp;quot; _N &amp;quot; observations&amp;quot;
There are 70262 observations

. format tstart %6.5f                                                                     

. format tstop %6.5f

. format weight_c %6.5f

. list failcode patid status tstart tstop weight_c weight_t status if patid==17, ///
&amp;gt;          sepby(failcode) noobs 

  +------------------------------------------------------------------------------+
  | failcode   patid   status    tstart     tstop   weight_c   weight_t   status |
  |------------------------------------------------------------------------------|
  |  relapse      17     died   0.00000   2.28884    1.00000          1     died |
  |  relapse      17     died   2.28884   2.31622    0.99000          1     died |
  |  relapse      17     died   2.31622   2.32717    0.98497          1     died |
  |  relapse      17     died   2.32717   2.36003    0.97992          1     died |
  |  relapse      17     died   2.36003   2.55441    0.91392          1     died |
  |  relapse      17     died   2.55441   2.65845    0.89843          1     died |
  |  relapse      17     died   2.65845   2.89938    0.85142          1     died |
  |  relapse      17     died   2.89938   3.02806    0.80937          1     died |
  |  relapse      17     died   3.02806   3.18960    0.76176          1     died |
  |  relapse      17     died   3.18960   3.26626    0.74578          1     died |
  |  relapse      17     died   3.26626   3.62765    0.63847          1     died |
  |  relapse      17     died   3.62765   3.89870    0.59519          1     died |
  |  relapse      17     died   3.89870   3.97536    0.57881          1     died |
  |  relapse      17     died   3.97536   4.10951    0.55124          1     died |
  |  relapse      17     died   4.10951   4.39425    0.51163          1     died |
  |  relapse      17     died   4.39425   4.50103    0.47714          1     died |
  |  relapse      17     died   4.50103   4.69815    0.45968          1     died |
  |  relapse      17     died   4.69815   5.08419    0.37101          1     died |
  |  relapse      17     died   5.08419   5.22656    0.32235          1     died |
  |  relapse      17     died   5.22656   5.33607    0.30995          1     died |
  |  relapse      17     died   5.33607   5.97673    0.22772          1     died |
  |  relapse      17     died   5.97673   6.27515    0.20170          1     died |
  |------------------------------------------------------------------------------|
  |     died      17     died   0.00000   2.28884    1.00000          1     died |
  +------------------------------------------------------------------------------+

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After using &lt;code&gt;stcrprep&lt;/code&gt; the number of rows has increased from 1977 to 70262. The rows have been divided based on the failure of the newly created variable &lt;code&gt;failcode&lt;/code&gt;. This variable will be used to fit different models depending on the event of interest. The variables &lt;code&gt;patid&lt;/code&gt; and &lt;code&gt;status&lt;/code&gt; are the same as in the non expanded data. The variables &lt;code&gt;tstart&lt;/code&gt; and &lt;code&gt;tstop&lt;/code&gt; give the times an individual starts and stops being at risk. They
change within an individual when their weight, defined by variable &lt;code&gt;weight_c&lt;/code&gt;, changes value. The &lt;code&gt;weight_t&lt;/code&gt; gives the weights when there is left trunction. As there is no left truncation in this data, it takes the value 1 for all subjects at all times.&lt;/p&gt;

&lt;p&gt;When &lt;code&gt;failcode==1&lt;/code&gt; this corresponds to when a relapse is the event of interest. As
the subject with &lt;code&gt;patid==17&lt;/code&gt; died after 2.29 years (i.e. had a competing event), they
are initially at risk until this time and they should receive a weight of 1 in the analysis.
After their death they are still kept in the risk set, but their weight decreases. The
decrease is based on the conditional probability of being censored which is estimated
using a non-parametric (Kaplan-Meier) estimate of the censoring distribution. The
weights only change at times when there is a failure for the event of interest and the
value of censoring distribution has changed.&lt;/p&gt;

&lt;p&gt;When &lt;code&gt;failcode==2&lt;/code&gt; this corresponds to when death is the event of interest. Since
this patient experienced the event of interest (i.e. they died) rather than the competing event, they only require one row of data.&lt;/p&gt;

&lt;p&gt;We can use &lt;code&gt;sts graph&lt;/code&gt; to give a plot of the cause-specific CIF. We first need to &lt;code&gt;stset&lt;/code&gt;
the data utilizing the information on the weights contained in variable &lt;code&gt;weights_c&lt;/code&gt; by
specifiying &lt;code&gt;iweights&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. gen event = status == failcode

. stset tstop [iw=weight_c], failure(event) enter(tstart) noshow                                          // stset using weights

     failure event:  event != 0 &amp;amp; event &amp;lt; .
obs. time interval:  (0, tstop]
 enter on or after:  time tstart
 exit on or before:  failure
            weight:  [iweight=weight_c]

------------------------------------------------------------------------------
     70,262  total observations
          0  exclusions
------------------------------------------------------------------------------
     70,262  observations remaining, representing
      1,141  failures in single-record/single-failure data
 13,820.402  total analysis time at risk and under observation
                                                at risk from t =         0
                                     earliest observed entry t =         0
                                          last observed exit t =  8.454483

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We first create the variable, &lt;code&gt;event&lt;/code&gt;. This is defined as 1 if the event of interest occurs
and zero otherwise. As we have split time data, we need to give information on the start
time (&lt;code&gt;tstart&lt;/code&gt;) and stop time (&lt;code&gt;tstop&lt;/code&gt;) of each row of data.
We use &lt;code&gt;sts graph&lt;/code&gt; in the usual way, but use the failure option as we are interested
in the probability of relapse as opposed to the probability of not having a relapse (which
includes the probability of death). For example, the cause-specific CIF for relapse can
be plotted as follows,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. sts graph if failcode==1, by(score) failure ///
&amp;gt;         ytitle(&amp;quot;Probability of Relapse&amp;quot;) ///
&amp;gt;         xtitle(&amp;quot;Years since transplanation&amp;quot;) ///
&amp;gt;         ylabel(0(0.1)0.5, angle(h) format(%3.1f)) ///
&amp;gt;         legend(order(1 &amp;quot;Low Risk&amp;quot; 2 &amp;quot;Medium Risk&amp;quot; 3 &amp;quot;High Risk&amp;quot;) ///
&amp;gt;         cols(1) ring(0) pos(5)) ///
&amp;gt;         scheme(sj) name(cif_relapse, replace)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/statasvg/stcrprep_cif1.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Note that the lines are extended to the maximum censoring time in each group, rather than the maximum event time.
Alternatively, &lt;code&gt;sts gen&lt;/code&gt; can be used to generate the cause-specific CIF and this can be
plotted with appropriate if statements to control the maximum follow-up time for each line.&lt;/p&gt;

&lt;p&gt;It is also possible to list the CIF at specific time points using &lt;code&gt;sts list&lt;/code&gt;. For example, the cause-specific CIF at 1 and 5 years by risk group and for each cause can be obtained as follows,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. sts list, at(1 5) failure by(failcode score)    

              Beg.                      Failure       Std.
    Time     Total     Fail             Function     Error     [95% Conf. Int.]
-------------------------------------------------------------------------------
relapse Low risk 
       1   348.001       38              0.0959    0.0148     0.0707    0.1295
       5   94.7875       36              0.2268    0.0250     0.1821    0.2805
relapse Medium risk 
       1   1125.93      225              0.1636    0.0100     0.1451    0.1843
       5   268.081      100              0.2594    0.0131     0.2347    0.2861
relapse High risk 
       1   116.387       39              0.2417    0.0338     0.1827    0.3156
       5         6       10              0.3306    0.0410     0.2574    0.4181
died Low risk 
       1   306.828       81              0.2032    0.0202     0.1669    0.2462
       5   94.9111       10              0.2368    0.0223     0.1964    0.2839
died Medium risk 
       1   915.771      441              0.3189    0.0126     0.2950    0.3442
       5   209.617       70              0.3829    0.0137     0.3566    0.4104
died High risk 
       1   84.7723       73              0.4494    0.0392     0.3764    0.5296
       5         6        7              0.5160    0.0452     0.4310    0.6071
-------------------------------------------------------------------------------
Note: Failure function is calculated over full data and evaluated at indicated
      times; it is not calculated from aggregates shown at left.

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we can test for differences in the cause-specific CIF using &lt;code&gt;sts test&lt;/code&gt;. Note that is slightly different to the modified log rank test defined by Gray (1988).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-stata&#34;&gt;. sts test score if failcode==1


Log-rank test for equality of survivor functions

            |   Events         Events
score       |  observed       expected
------------+-------------------------
Low risk    |        79          99.64
Medium risk |       328         324.33
High risk   |        49          32.04
------------+-------------------------
Total       |       456         456.00

                  chi2(2) =      13.37
                  Pr&amp;gt;chi2 =     0.0012

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;de Wreede, L.; Fiocco, M. &amp;amp; Putter, H. &lt;code&gt;mstate&lt;/code&gt;: An R package for the analysis of competing risks and multi-state models. &lt;em&gt;Journal of Statistical Software&lt;/em&gt; 2011;&lt;strong&gt;38&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Gray, R. A class of K-sample tests for comparing the cumulative incidence of a competing risk. &lt;em&gt;The Annals of Statistics&lt;/em&gt; 1988;&lt;strong&gt;16&lt;/strong&gt;:1141-1154.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
